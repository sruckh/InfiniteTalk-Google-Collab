{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# 1. Clone Repository (Safe to re-run)\n",
    "if not os.path.exists(\"InfiniteTalk\"):\n",
    "    !git clone https://github.com/MeiGen-AI/InfiniteTalk\n",
    "else:\n",
    "    print(\"Repository already cloned.\")\n",
    "\n",
    "# 2. Enter Directory (Required after every restart)\n",
    "%cd InfiniteTalk\n",
    "\n",
    "# 3. Safe Dependency Installation\n",
    "py_ver = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "torch_ver = torch.__version__.split('+')[0]\n",
    "torch_ver_short = \".\".join(torch_ver.split(\".\")[:2])\n",
    "cuda_ver = torch.version.cuda.replace(\".\", \"\")[:3] if torch.version.cuda else \"cpu\"\n",
    "abi = \"TRUE\" if torch._C._GLIBCXX_USE_CXX11_ABI else \"FALSE\"\n",
    "\n",
    "print(f\"✅ Detected: Python={py_ver}, Torch={torch_ver}, CUDA={cuda_ver}, ABI={abi}\")\n",
    "print(\"Installing dependencies... (Fast mode enabled)\")\n",
    "\n",
    "# Install dependencies needed for extensions\n",
    "!pip install --no-cache-dir packaging ninja psutil wheel\n",
    "\n",
    "# Install Flash Attention 2 (Dynamic Wheel Selection)\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(\"✅ Flash Attention already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing Flash Attention...\")\n",
    "    \n",
    "    # CASE 1: Python 3.12 + PyTorch 2.9 (User Verified Environment)\n",
    "    if py_ver == \"cp312\" and torch_ver.startswith(\"2.9\"):\n",
    "        url = \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl\"\n",
    "    else:\n",
    "        # CASE 2: Fallback to guessing official URL for other versions\n",
    "        fa_v = \"2.8.3\"\n",
    "        cu_tag = f\"cu{cuda_ver[:2]}\" # e.g., cu12\n",
    "        wheel = f\"flash_attn-{fa_v}+{cu_tag}torch{torch_ver_short}cxx11abi{abi}-{py_ver}-{py_ver}-linux_x86_64.whl\"\n",
    "        url = f\"https://github.com/Dao-AILab/flash-attention/releases/download/v{fa_v}/{wheel}\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Attempting to install: {url}\")\n",
    "        res = os.system(f\"pip install --no-cache-dir {url}\")\n",
    "        if res != 0:\n",
    "            raise Exception(\"Pip install returned non-zero exit code\")\n",
    "        print(f\"✅ Successfully installed Flash Attention via wheel\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Wheel install failed: {e}\")\n",
    "        print(\"Falling back to standard pip install (this may trigger a slow build)...\")\n",
    "        !pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Install other dependencies from requirements.txt\n",
    "!sed -i 's/numpy>=1.23.5,<2/numpy>=1.23.5/' requirements.txt\n",
    "!pip install --no-cache-dir -r requirements.txt\n",
    "!pip install --no-cache-dir librosa huggingface_hub\n",
    "\n",
    "# Install system dependencies\n",
    "!apt-get update && apt-get install -y ffmpeg > /dev/null 2>&1\n",
    "\n",
    "print(\"\\n✅ Setup Complete! If you see a 'Restart Session' button, you can IGNORE it and try running the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.makedirs(\"weights\", exist_ok=True)\n\n# Download base model and InfiniteTalk weights\n!hf download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./weights/Wan2.1-I2V-14B-480P\n!hf download MeiGen-AI/InfiniteTalk --local-dir ./weights/InfiniteTalk\n\n# Download English audio encoder (DEFAULT)\n!hf download facebook/wav2vec2-base-960h --local-dir ./weights/wav2vec2-base-960h\n\n# Alternative: For Chinese audio support, uncomment the lines below:\n# !hf download TencentGameMate/chinese-wav2vec2-base --local-dir ./weights/chinese-wav2vec2-base\n# !hf download TencentGameMate/chinese-wav2vec2-base model.safetensors --revision refs/pr/1 --local-dir ./weights/chinese-wav2vec2-base"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Enable public link for Gradio\n!sed -i 's/demo.launch(server_name=\"0.0.0.0\", debug=True, server_port=8418)/demo.launch(server_name=\"0.0.0.0\", debug=True, server_port=8418, share=True)/' app.py\n\n# Launch Gradio interface with English audio encoder (DEFAULT)\n!python app.py --ckpt_dir weights/Wan2.1-I2V-14B-480P --wav2vec_dir 'weights/wav2vec2-base-960h' --infinitetalk_dir weights/InfiniteTalk/single/infinitetalk.safetensors --num_persistent_param_in_dit 0 --motion_frame 9\n\n# Alternative: For Chinese audio, uncomment below and comment the English line above:\n# !python app.py --ckpt_dir weights/Wan2.1-I2V-14B-480P --wav2vec_dir 'weights/chinese-wav2vec2-base' --infinitetalk_dir weights/InfiniteTalk/single/infinitetalk.safetensors --num_persistent_param_in_dit 0 --motion_frame 9"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}