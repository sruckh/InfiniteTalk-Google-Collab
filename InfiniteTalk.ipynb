{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import platform\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import sys\n",
    "import platform\n",
    "\n",
    "# 1. Clone Repository (Safe to re-run)\n",
    "if not os.path.exists(\"InfiniteTalk\"):\n",
    "    !git clone https://github.com/MeiGen-AI/InfiniteTalk\n",
    "else:\n",
    "    print(\"Repository already cloned.\")\n",
    "\n",
    "# 2. Enter Directory (Required after every restart)\n",
    "%cd InfiniteTalk\n",
    "\n",
    "# 3. Safe Dependency Installation\n",
    "py_ver = f\"cp{sys.version_info.major}{sys.version_info.minor}\"\n",
    "torch_ver = torch.__version__.split(\"+\")[0]\n",
    "torch_ver_short = \".\".join(torch_ver.split(\"Û”\")[:2])\n",
    "cuda_ver = torch.version.cuda.replace(\"Û”\", \"")[:3] if torch.version.cuda else \"cpu\"\n",
    "abi = \"TRUE\" if torch._C._GLIBCXX_USE_CXX11_ABI else \"FALSE\"\n",
    "\n",
    "print(f\"âœ… Detected: Python={py_ver}, Torch={torch_ver}, CUDA={cuda_ver}, ABI={abi}\")\n",
    "print(\"Installing dependencies... (Fast mode enabled)\")\n",
    "\n",
    "# Install dependencies needed for extensions\n",
    "!pip install --no-cache-dir packaging ninja psutil wheel\n",
    "\n",
    "# Install Flash Attention 2 (Dynamic Wheel Selection)\n",
    "try:\n",
    "    import flash_attn\n",
    "    print(\"âœ… Flash Attention already installed.\")\n",
    "except ImportError:\n",
    "    print(\"Installing Flash Attention...\")\n",
    "    \n",
    "    # CASE 1: Python 3.12 + PyTorch 2.9 (User Verified Environment)\n",
    "    if py_ver == \"cp312\" and torch_ver.startswith(\"2.9\"):\n",
    "        url = \"https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl\"\n",
    "    else:\n",
    "        # CASE 2: Fallback to guessing official URL for other versions\n",
    "        fa_v = \"2.8.3\"\n",
    "        cu_tag = f\"cu{cuda_ver[:2]}\" # e.g., cu12\n",
    "        wheel = f\"flash_attn-{fa_v}+{cu_tag}torch{torch_ver_short}cxx11abi{abi}-{py_ver}-{py_ver}-linux_x86_64.whl\"\n",
    "        url = f\"https://github.com/Dao-AILab/flash-attention/releases/download/v{fa_v}/{wheel}\"\n",
    "\n",
    "    try:\n",
    "        print(f\"Attempting to install: {url}\")\n",
    "        res = os.system(f\"pip install --no-cache-dir {url}\")\n",
    "        if res != 0:\n",
    "            raise Exception(\"Pip install returned non-zero exit code\")\n",
    "        print(f\"âœ… Successfully installed Flash Attention via wheel\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Wheel install failed: {e}\")\n",
    "        print(\"Falling back to standard pip install (this may trigger a slow build)...\")\n",
    "        !pip install flash-attn --no-build-isolation\n",
    "\n",
    "# Install other dependencies from requirements.txt\n",
    "!sed -i 's/numpy>=1.23.5,<2/numpy>=1.23.5/' requirements.txt\n",
    "!pip install --no-cache-dir -r requirements.txt\n",
    "!pip install --no-cache-dir librosa huggingface_hub\n",
    "\n",
    "# Install system dependencies\n",
    "!apt-get update && apt-get install -y ffmpeg > /dev/null 2>&1\n",
    "\n",
    "print(\"\\nâœ… Setup Complete! If you see a 'Restart Session' button, you can IGNORE it and try running the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"weights\", exist_ok=True)\n",
    "os.makedirs(\"weights/InfiniteTalk/quant_models\", exist_ok=True)\n",
    "\n",
    "# PREVENT DOUBLE DISK USAGE: Set HF cache to weights folder\n",
    "os.environ[\"HF_HUB_CACHE\"] = os.path.join(os.getcwd(), \"weights/.cache\")\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"true\"\n",
    "\n",
    "print(\"Downloading models (Optimized for space)...\")\n",
    "\n",
    "# 1. Download base model (Wan2.1-I2V-14B-480P)\n",
    "!hf download Wan-AI/Wan2.1-I2V-14B-480P --local-dir ./weights/Wan2.1-I2V-14B-480P --local-dir-use-symlinks False\n",
    "\n",
    "# 2. Download FP8 Quantized Weights (SAVED DISK & VRAM)\n",
    "# These are smaller and required for Tesla T4 (16GB VRAM)\n",
    "print(\"Downloading FP8 weights for Tesla T4 compatibility...\")\n",
    "!wget -O weights/InfiniteTalk/quant_models/infinitetalk_single_fp8.safetensors https://huggingface.co/MeiGen-AI/InfiniteTalk/resolve/main/quant_models/infinitetalk_single_fp8.safetensors\n",
    "!wget -O weights/InfiniteTalk/quant_models/infinitetalk_multi_fp8.safetensors https://huggingface.co/MeiGen-AI/InfiniteTalk/resolve/main/quant_models/infinitetalk_multi_fp8.safetensors\n",
    "\n",
    "# 3. Download English audio encoder\n",
    "!hf download facebook/wav2vec2-base-960h --local-dir ./weights/wav2vec2-base-960h --local-dir-use-symlinks False\n",
    "\n",
    "print(\"âœ… Download Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable public link for Gradio\n",
    "!sed -i 's/demo.launch(server_name=\"0.0.0.0\", debug=True, server_port=8418)/demo.launch(server_name=\"0.0.0.0\", debug=True, server_port=8418, share=True)/' app.py\n",
    "\n",
    "print(\"ðŸš€ Launching in FP8 Mode (Compatible with Tesla T4 16GB)\")\n",
    "\n",
    "# Launch Gradio interface with FP8 Optimization enabled\n",
    "!python app.py \
",
    "    --ckpt_dir weights/Wan2.1-I2V-14B-480P \
",
    "    --wav2vec_dir 'weights/wav2vec2-base-960h' \
",
    "    --infinitetalk_dir weights/InfiniteTalk/single/infinitetalk.safetensors \
",
    "    --quant fp8 \
",
    "    --quant_dir weights/InfiniteTalk/quant_models/infinitetalk_single_fp8.safetensors \
",
    "    --num_persistent_param_in_dit 0 \
",
    "    --motion_frame 9"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}